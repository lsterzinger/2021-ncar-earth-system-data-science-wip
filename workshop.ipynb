{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NCAR Earth System Data Science WIP Talk\n",
    "__This presentation is based on work I did during the [NCAR Summer Internship in Parallel Computational Science (SIParCS) program](https://www2.cisl.ucar.edu/siparcs)__\n",
    "### Lucas Sterzinger -- Atmospheric Science PhD Candidate at UC Davis\n",
    "* [Twitter](https://twitter.com/lucassterzinger)\n",
    "* [GitHub](https://github.com/lsterzinger)\n",
    "* [Website](https://lucassterzinger.com)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Motivation:\n",
    "* NetCDF is not cloud optimized\n",
    "* Other formats, like Zarr, aim to "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What do I mean when I say \"Cloud Optimized\"?\n",
    "![Move to cloud diagram](images/cloud-move.png)\n",
    "\n",
    "In traditional scientific workflows, data is archived in a repository and downloaded to a separate computer for analysis (left). However, datasets are becoming much too large to fit on personal computers, and transferring full datasets from an archive to a seperate machine can use lots of bandwidth.\n",
    "\n",
    "In a cloud environment, the data can live in object storage (e.g. AWS S3), and analysis can be done in an adjacent compute instances, allowing for low-latency and high-bandwith access to the dataset.\n",
    "\n",
    "## Why NetCDF doesn't work well in this workflow\n",
    "\n",
    "NetCDF is probably the most common binary data format for atmospheric/earth sciences, and has a lot of official and community support. However, the NetCDF format/API requires either a) loading the entire dataset in order to access the header/metadata and retreive a chunk of data.\n",
    "\n",
    "![NetCDF File Object](images/single_file_object.png)\n",
    "\n",
    "## The Zarr Solution\n",
    "The [Zarr data format](https://zarr.readthedocs.io/en/stable/) alleviates this problem by storing the metadata and chunks in seperate files that can be accessed as-needed and in parallel.\n",
    "\n",
    "![Zarr](images/zarr.png)\n",
    "\n",
    "## _However_\n",
    "While Zarr proves to be very good for this cloud-centric workflow, most cloud-available data is currently only available in NetCDF/HDF5/GRIB2 format. While it would be _wonderful_ if all this data converted to Zarr overnight, in the meantime it would be great if there was a way to use some of the Zarr spec, right?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introducting `fsspec-reference-maker`\n",
    "[Github page](https://github.com/intake/fsspec-reference-maker)\n",
    "\n",
    "`fsspec-reference-maker` works by analysing the NetCDF header/metadata info, extracting byte-ranges for each variable chunk, and creating a Zarr-spec metadata file. This file is plaintext and can opened and analyzed with xarray very quickly. When a user requests a certain chunk of data, the NetCDF4 API is bypassed entirely and the Zarr API is used to extract the specified byte-range.\n",
    "\n",
    "![reference-maker vs zarr](images/referencemaker_v_zarr.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Let's try it out!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import `fsspec-reference-maker` and make sure it's at the latest version (`0.0.3` at the time of writing)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import fsspec_reference_maker\n",
    "fsspec_reference_maker.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr\n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import fsspec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup an S3 filesystem for listing GOES files on S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "flist = fs.glob(\"s3://noaa-goes16/ABI-L2-SSTF/2020/210/*/*.nc\")\n",
    "# flist = fs.glob(\"s3://noaa-goes16/ABI-L2-MCMIPC/2021/049/*/*.nc\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepend `s3://` to the URLS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "flist = ['s3://' + f for f in flist]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start a dask cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import dask.bag as db\n",
    "flist_bag = db.from_sequence(flist, npartitions=len(flist))\n",
    "flist_bag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definte function to return a reference dictionary for a given S3 file URL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gen_ref(f):\n",
    "    so = dict(\n",
    "        mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"none\"\n",
    "    )\n",
    "\n",
    "    with fsspec.open(f, **so) as infile:\n",
    "        return SingleHdf5ToZarr(infile, f, inline_threshold=300).translate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Map `gen_ref` to each member of `flist_bag` and compute\n",
    "_Note: if running interactively on Binder, this will take a while since only one worker is available and the references will have to be generated in serial_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time dicts = flist_bag.map(gen_ref).compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The individual dictionaries can be saved as JSON files if desired"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "d = dicts[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "d['templates']['u']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s3://noaa-goes16/ABI-L2-SSTF/2020/210/00/OR_ABI-L2-SSTF-M6_G16_s20202100000205_e20202100059513_c20202100105456.nc'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ujson\n",
    "for d in dicts:\n",
    "    # Generate name from corresponding URL:\n",
    "    # Grab URL, strip everything but the filename, \n",
    "    # and replace .nc with .json\n",
    "    name = d['templates']['u'].split('/')[-1].replace('.nc', '.json')\n",
    "\n",
    "    with open(name, 'w') as outf:\n",
    "        outf.write(ujson.dumps(d))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use `MultiZarrToZarr` to combine the 24 individual references into a single reference\n",
    "In this example we passed a list of reference dictionaries, but you can also give it a list of `.json` filepaths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    dicts,\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    xarray_open_kwargs={\n",
    "        \"decode_cf\" : False,\n",
    "        \"mask_and_scale\" : False,\n",
    "        \"decode_times\" : False,\n",
    "        \"decode_timedelta\" : False,\n",
    "        \"use_cftime\" : False,\n",
    "        \"decode_coords\" : False\n",
    "    },\n",
    "    xarray_concat_args={'dim' : 't'}\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "References can be saved to a file (`combined.json`) or passed back as a dictionary (`mzz_dict`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mzz.translate('./combined.json')\n",
    "# mzz_dict = mzz.translate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read the referenced files with `fsspec` and `xarray`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import metpy\n",
    "import cartopy.crs as ccrs\n",
    "import hvplot.xarray"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use metpy's `parse_cf()` to generate projection information for future plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fs2 = fsspec.filesystem('reference', fo=\"./example_jsons/combined.json\", remote_protocol='s3', remote_options=dict(anon=True), skip_instance_cache=True)\n",
    "ds = xr.open_dataset(fs2.get_mapper(\"\"), engine='zarr').metpy.parse_cf()\n",
    "ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use metpy to calculate lat/lon based on the GOES projection grid, and rename time dimension (for better plotting with hvplot)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = ds.metpy.assign_latitude_longitude()\n",
    "ds['t'].attrs['long_name'] = 'Time'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mask_lat = (ds.latitude > 30) & (ds.latitude < 60)\n",
    "mask_lon = (ds.longitude > -90) & (ds.longitude < -60)\n",
    "\n",
    "\n",
    "sst = ds.SST.where(mask_lat & mask_lon, drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sst.hvplot.quadmesh(\n",
    "    'x', 'y', groupby='t',\n",
    "    crs = sst.metpy.cartopy_crs, projection=ccrs.Orthographic(-75,30),\n",
    "    project=True, rasterize=True, coastline=True\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sst.mean(dim='t').hvplot.quadmesh(\n",
    "    'x', 'y',\n",
    "    crs = sst.metpy.cartopy_crs, projection=ccrs.Orthographic(-75,30),\n",
    "    project=True, rasterize=True, coastline=True\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d853cbf2f35f45a59f79ca5e397d8dd1594080251b0b51418fe33f5fb0138a7a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('fsspec-reference-maker': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}